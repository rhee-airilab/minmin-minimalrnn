{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:24:05.253664Z",
     "start_time": "2017-12-12T13:24:04.493518Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "from __future__ import print_function, division, absolute_import\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:24:05.372975Z",
     "start_time": "2017-12-12T13:24:05.255061Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -fr logdir\n",
    "!mkdir -p logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:24:05.811621Z",
     "start_time": "2017-12-12T13:24:05.506894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist.input_data \\\n",
    "  import read_data_sets\n",
    "\n",
    "mnist = read_data_sets('./mnist', one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:24:06.131307Z",
     "start_time": "2017-12-12T13:24:06.128673Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_UNITS = 28\n",
    "BATCH_SIZE = 128\n",
    "MAX_SEQ_LEN = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:24:06.139750Z",
     "start_time": "2017-12-12T13:24:06.132935Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loop_count = mnist.train.num_examples // BATCH_SIZE\n",
    "test_loop_count  = mnist.test.num_examples // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:24:06.148736Z",
     "start_time": "2017-12-12T13:24:06.141258Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from minimalrnn import MinimalRNNCell\n",
    "from tensorflow.python.layers.layers import conv1d,max_pooling1d,dropout,batch_normalization\n",
    "\n",
    "class MnistRnn:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, args, inputs, labels):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        training    = tf.Variable(False,name='training',dtype=tf.bool)\n",
    "        global_step = tf.Variable(1,name='global_step',dtype=tf.int64)\n",
    "        rnn_dropout_rate = tf.Variable(0.0,name='dropout_rate',dtype=tf.float32)\n",
    "        cnn_dropout_rate = tf.Variable(0.0,name='dropout_rate',dtype=tf.float32)\n",
    "\n",
    "        step_update = tf.placeholder(tf.int64,None,name='step_update')\n",
    "        rnn_dropout_update = tf.placeholder(tf.float32,None,name='rnn_dropout_update')\n",
    "        cnn_dropout_update = tf.placeholder(tf.float32,None,name='cnn_dropout_update')\n",
    "\n",
    "        def my_phi_initializer(inputs, num_outputs, **kwargs):\n",
    "            \n",
    "            print(('my_phi_initializer','inputs.shape',inputs.get_shape().as_list()))\n",
    "            \n",
    "            def my_phi(inputs):\n",
    "                \n",
    "                cnn_keep_prob   = tf.cond(training,lambda: 1.0 - cnn_dropout_rate,lambda: 1.0)\n",
    "\n",
    "                input_units = inputs.get_shape().as_list()[-1]\n",
    "                layer   = tf.reshape(inputs, [-1,input_units,1])\n",
    "\n",
    "                layer   = conv1d(layer, num_outputs//3, 5, 1, activation=tf.nn.relu)\n",
    "                layer   = max_pooling1d(layer, 3, 2)\n",
    "                layer   = conv1d(layer, num_outputs, 3, 1, activation=tf.nn.relu)\n",
    "                layer   = conv1d(layer, num_outputs, 3, 1, activation=tf.nn.relu)\n",
    "                layer   = conv1d(layer, num_outputs//2, 3, 1, activation=tf.nn.relu)\n",
    "                layer   = max_pooling1d(layer, 3, 2)\n",
    "\n",
    "                layer   = conv1d(layer, num_outputs, 1, 1, activation=tf.nn.relu)\n",
    "                layer   = dropout(layer, rate=1.0-cnn_keep_prob)\n",
    "                if args.use_batch_norm:\n",
    "                    layer   = tf.layers.batch_normalization(layer, training=training)\n",
    "                layer   = conv1d(layer, num_outputs, 1, 1, activation=tf.nn.relu)\n",
    "\n",
    "                layer   = tf.reduce_mean(layer, axis=1, keep_dims=True)\n",
    "                layer   = tf.squeeze(layer)\n",
    "\n",
    "                print(('my_phi','layer.shape',layer.get_shape().as_list()))\n",
    "\n",
    "                return layer\n",
    "            \n",
    "            \n",
    "            return my_phi\n",
    "\n",
    "\n",
    "        cell = MinimalRNNCell(args.num_hidden_units,\n",
    "                              phi_initializer=my_phi_initializer if args.use_custom_phi else None)\n",
    "\n",
    "        rnn_keep_prob   = tf.cond(training,lambda: 1.0 - rnn_dropout_rate,lambda: 1.0)\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "            cell,\n",
    "            state_keep_prob=rnn_keep_prob,\n",
    "            variational_recurrent=args.use_variational_dropout,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        \n",
    "        sequence_length = [MAX_SEQ_LEN] * BATCH_SIZE\n",
    "        \n",
    "        last, states = tf.nn.dynamic_rnn(\n",
    "            cell,\n",
    "            inputs,\n",
    "            sequence_length=sequence_length,\n",
    "            dtype=tf.float32)\n",
    "        rnn_output = last[:,MAX_SEQ_LEN-1,:]\n",
    "        outputs    = tf.layers.dense(rnn_output, 10)\n",
    "        \n",
    "        loss       = tf.losses.sparse_softmax_cross_entropy(\n",
    "            labels,\n",
    "            outputs)\n",
    "\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            optimize   = tf.train.AdamOptimizer(learning_rate=0.001). \\\n",
    "                minimize(loss)\n",
    "\n",
    "        preds      = tf.argmax(outputs, axis=1)\n",
    "        errors     = tf.count_nonzero(labels - preds)\n",
    "        accuracy   = 1.0 - tf.cast(errors,tf.float32) / \\\n",
    "                        tf.cast(tf.size(preds), tf.float32)\n",
    "        \n",
    "        with tf.control_dependencies([\n",
    "            tf.assign(training, True),\n",
    "            tf.assign(global_step, step_update),\n",
    "            tf.assign(rnn_dropout_rate, rnn_dropout_update),\n",
    "            tf.assign(cnn_dropout_rate, cnn_dropout_update),\n",
    "        ]):\n",
    "            self.train_mode = tf.constant(1)\n",
    "            \n",
    "        with tf.control_dependencies([\n",
    "            tf.assign(training, False)\n",
    "        ]):\n",
    "            self.test_mode  = tf.constant(1)\n",
    "\n",
    "        self.step_update = step_update\n",
    "        self.rnn_dropout_update = rnn_dropout_update\n",
    "        self.cnn_dropout_update = cnn_dropout_update\n",
    "        self.inputs   = inputs\n",
    "        self.labels   = labels\n",
    "        self.outputs  = outputs\n",
    "        self.loss     = loss\n",
    "        self.optimize = optimize\n",
    "        self.accuracy = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:24:06.261738Z",
     "start_time": "2017-12-12T13:24:06.236746Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(args, sess, model, max_epochs, train_writer=None, test_writer=None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    step = sess.run(tf.train.get_global_step())\n",
    "\n",
    "    for ep in range(max_epochs):\n",
    "\n",
    "        for i in range(train_loop_count):\n",
    "\n",
    "            sess.run(model.train_mode, {\n",
    "                model.step_update: step,\n",
    "                model.rnn_dropout_update: args.rnn_dropout_rate,\n",
    "                model.cnn_dropout_update: args.cnn_dropout_rate})\n",
    "\n",
    "            offs = i * BATCH_SIZE\n",
    "            batch_input = mnist.train.images[offs:offs+BATCH_SIZE,:]. \\\n",
    "                            reshape([BATCH_SIZE, MAX_SEQ_LEN, INPUT_UNITS])\n",
    "            batch_label = mnist.train.labels[offs:offs+BATCH_SIZE]\n",
    "            t_start     = time.time()\n",
    "            _, loss, accuracy = sess.run(\n",
    "                [model.optimize, model.loss, model.accuracy],\n",
    "                 feed_dict = {\n",
    "                     model.inputs: batch_input,\n",
    "                     model.labels: batch_label })\n",
    "            t_elapsed     = time.time() - t_start\n",
    "            step += 1\n",
    "            \n",
    "            if train_writer:\n",
    "                summary = tf.Summary(\n",
    "                    value = [\n",
    "                        tf.Summary.Value(tag='accuracy',simple_value=accuracy),\n",
    "                        tf.Summary.Value(tag='loss',simple_value=loss),\n",
    "                        tf.Summary.Value(tag='elapsed',simple_value=t_elapsed),\n",
    "                    ])\n",
    "                train_writer.add_summary(summary, global_step=step)\n",
    "                train_writer.flush()\n",
    "                \n",
    "            if step % 251 == 0:\n",
    "                print('[train] ep {:d}, step {:d}, accu {:.5f}, loss {:.5f} [elapsed {:.5f}]'.format(\n",
    "                    ep + 1, step, accuracy, loss, t_elapsed))\n",
    "                \n",
    "        test_accuracies = []\n",
    "        test_elapsed    = []\n",
    "        \n",
    "        sess.run(model.test_mode)\n",
    "\n",
    "        for i in range(test_loop_count):\n",
    "            offs = i * BATCH_SIZE\n",
    "            batch_input = mnist.test.images[offs:offs+BATCH_SIZE,:]. \\\n",
    "                            reshape([BATCH_SIZE, MAX_SEQ_LEN, INPUT_UNITS])\n",
    "            batch_label = mnist.test.labels[offs:offs+BATCH_SIZE]\n",
    "            t_start   = time.time()\n",
    "            accuracy, = sess.run([model.accuracy],\n",
    "                                 feed_dict = {\n",
    "                                     model.inputs: batch_input,\n",
    "                                     model.labels: batch_label})\n",
    "            t_elapsed = time.time() - t_start\n",
    "            test_accuracies.append(accuracy)\n",
    "            test_elapsed.append(t_elapsed)\n",
    "\n",
    "        mean_accuracy = np.mean(test_accuracies)\n",
    "        mean_elapsed  = np.mean(test_elapsed)\n",
    "        \n",
    "        if test_writer:\n",
    "            summary = tf.Summary(\n",
    "                value = [\n",
    "                    tf.Summary.Value(tag='accuracy',simple_value=mean_accuracy),\n",
    "                    tf.Summary.Value(tag='elapsed',simple_value=mean_elapsed),\n",
    "                ])\n",
    "            test_writer.add_summary(summary, global_step=step)\n",
    "            test_writer.flush()\n",
    "\n",
    "        print(' [test] ep {:d}, step {:d}, accu {:.5f} [elapsed {:.5f}]'.format(\n",
    "            ep + 1, step, mean_accuracy, mean_elapsed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "HyperParams = namedtuple('HyperParams',[\n",
    "    'num_hidden_units',\n",
    "    'rnn_dropout_rate',\n",
    "    'cnn_dropout_rate',\n",
    "    'use_custom_phi',\n",
    "    'use_batch_norm',\n",
    "    'use_variational_dropout'], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hp_configs = [\n",
    "    HyperParams(num_hidden_units=31,\n",
    "                rnn_dropout_rate=0.0,\n",
    "                cnn_dropout_rate=0.0,\n",
    "                use_custom_phi=False,\n",
    "                use_batch_norm=False,\n",
    "                use_variational_dropout=True),\n",
    "    HyperParams(num_hidden_units=15,\n",
    "                rnn_dropout_rate=0.0,\n",
    "                cnn_dropout_rate=0.0,\n",
    "                use_custom_phi=False,\n",
    "                use_batch_norm=False,\n",
    "                use_variational_dropout=True),\n",
    "    HyperParams(num_hidden_units=31,\n",
    "                rnn_dropout_rate=0.0,\n",
    "                cnn_dropout_rate=0.0,\n",
    "                use_custom_phi=True,\n",
    "                use_batch_norm=False,\n",
    "                use_variational_dropout=True),\n",
    "    HyperParams(num_hidden_units=15,\n",
    "                rnn_dropout_rate=0.0,\n",
    "                cnn_dropout_rate=0.0,\n",
    "                use_custom_phi=True,\n",
    "                use_batch_norm=False,\n",
    "                use_variational_dropout=True),\n",
    "    HyperParams(num_hidden_units=61,\n",
    "                rnn_dropout_rate=0.0,\n",
    "                cnn_dropout_rate=0.0,\n",
    "                use_custom_phi=False,\n",
    "                use_batch_norm=False,\n",
    "                use_variational_dropout=True),\n",
    "    HyperParams(num_hidden_units=61,\n",
    "                rnn_dropout_rate=0.0,\n",
    "                cnn_dropout_rate=0.0,\n",
    "                use_custom_phi=True,\n",
    "                use_batch_norm=False,\n",
    "                use_variational_dropout=True),\n",
    "    HyperParams(num_hidden_units=91,\n",
    "                rnn_dropout_rate=0.0,\n",
    "                cnn_dropout_rate=0.0,\n",
    "                use_custom_phi=False,\n",
    "                use_batch_norm=False,\n",
    "                use_variational_dropout=True),\n",
    "    HyperParams(num_hidden_units=91,\n",
    "                rnn_dropout_rate=0.0,\n",
    "                cnn_dropout_rate=0.0,\n",
    "                use_custom_phi=True,\n",
    "                use_batch_norm=False,\n",
    "                use_variational_dropout=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib import quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hp_signature(hp):\n",
    "#     return '/'.join([quote(str(k)) + '=' + quote(str(v)) for k,v in args._asdict().items()])\n",
    "    return '/'.join([quote(str(v)) for k,v in args._asdict().items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:24:06.817908Z",
     "start_time": "2017-12-12T13:24:06.264586Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training: 31/0.0/0.0/False/False/True\n",
      "[train] ep 1, step 251, accu 0.54688, loss 1.59292 [elapsed 0.01381]\n",
      " [test] ep 1, step 430, accu 0.70192 [elapsed 0.00647]\n",
      "[train] ep 2, step 502, accu 0.69531, loss 1.01491 [elapsed 0.01304]\n",
      "[train] ep 2, step 753, accu 0.75000, loss 0.70770 [elapsed 0.01305]\n",
      " [test] ep 2, step 859, accu 0.82833 [elapsed 0.00652]\n",
      "[train] ep 3, step 1004, accu 0.88281, loss 0.49037 [elapsed 0.01315]\n",
      "[train] ep 3, step 1255, accu 0.92188, loss 0.35902 [elapsed 0.01330]\n",
      " [test] ep 3, step 1288, accu 0.86729 [elapsed 0.00645]\n",
      "[train] ep 4, step 1506, accu 0.89062, loss 0.32810 [elapsed 0.01335]\n",
      " [test] ep 4, step 1717, accu 0.89323 [elapsed 0.00678]\n",
      "[train] ep 5, step 1757, accu 0.87500, loss 0.38172 [elapsed 0.01427]\n",
      "[train] ep 5, step 2008, accu 0.91406, loss 0.34555 [elapsed 0.01328]\n",
      " [test] ep 5, step 2146, accu 0.90825 [elapsed 0.00634]\n",
      "[train] ep 6, step 2259, accu 0.89062, loss 0.34980 [elapsed 0.01295]\n",
      "[train] ep 6, step 2510, accu 0.95312, loss 0.16060 [elapsed 0.01307]\n",
      " [test] ep 6, step 2575, accu 0.91997 [elapsed 0.00636]\n",
      "[train] ep 7, step 2761, accu 0.96094, loss 0.17159 [elapsed 0.01313]\n",
      " [test] ep 7, step 3004, accu 0.92919 [elapsed 0.00664]\n",
      "[train] ep 8, step 3012, accu 0.94531, loss 0.19045 [elapsed 0.01347]\n",
      "[train] ep 8, step 3263, accu 0.96094, loss 0.16412 [elapsed 0.01318]\n",
      " [test] ep 8, step 3433, accu 0.93530 [elapsed 0.00632]\n",
      "[train] ep 9, step 3514, accu 0.92969, loss 0.20749 [elapsed 0.01311]\n",
      "[train] ep 9, step 3765, accu 0.94531, loss 0.17414 [elapsed 0.01353]\n",
      " [test] ep 9, step 3862, accu 0.94050 [elapsed 0.00632]\n",
      "[train] ep 10, step 4016, accu 0.88281, loss 0.29349 [elapsed 0.01330]\n",
      "[train] ep 10, step 4267, accu 0.96094, loss 0.12489 [elapsed 0.01331]\n",
      " [test] ep 10, step 4291, accu 0.94661 [elapsed 0.00634]\n",
      "start training: 15/0.0/0.0/False/False/True\n",
      "[train] ep 1, step 251, accu 0.34375, loss 1.95137 [elapsed 0.00818]\n",
      " [test] ep 1, step 430, accu 0.53556 [elapsed 0.00390]\n",
      "[train] ep 2, step 502, accu 0.44531, loss 1.58608 [elapsed 0.00805]\n",
      "[train] ep 2, step 753, accu 0.66406, loss 1.10739 [elapsed 0.00820]\n",
      " [test] ep 2, step 859, accu 0.69511 [elapsed 0.00384]\n",
      "[train] ep 3, step 1004, accu 0.67969, loss 0.98678 [elapsed 0.00847]\n",
      "[train] ep 3, step 1255, accu 0.82812, loss 0.68385 [elapsed 0.00839]\n",
      " [test] ep 3, step 1288, accu 0.77063 [elapsed 0.00389]\n",
      "[train] ep 4, step 1506, accu 0.81250, loss 0.64338 [elapsed 0.00807]\n",
      " [test] ep 4, step 1717, accu 0.80970 [elapsed 0.00387]\n",
      "[train] ep 5, step 1757, accu 0.80469, loss 0.63740 [elapsed 0.00850]\n",
      "[train] ep 5, step 2008, accu 0.74219, loss 0.66654 [elapsed 0.01423]\n",
      " [test] ep 5, step 2146, accu 0.83243 [elapsed 0.00386]\n",
      "[train] ep 6, step 2259, accu 0.88281, loss 0.49361 [elapsed 0.00847]\n",
      "[train] ep 6, step 2510, accu 0.85938, loss 0.44423 [elapsed 0.00828]\n",
      " [test] ep 6, step 2575, accu 0.85136 [elapsed 0.00445]\n",
      "[train] ep 7, step 2761, accu 0.85938, loss 0.42637 [elapsed 0.00824]\n",
      " [test] ep 7, step 3004, accu 0.86168 [elapsed 0.00391]\n",
      "[train] ep 8, step 3012, accu 0.85156, loss 0.44370 [elapsed 0.00873]\n",
      "[train] ep 8, step 3263, accu 0.89844, loss 0.39605 [elapsed 0.00800]\n",
      " [test] ep 8, step 3433, accu 0.86979 [elapsed 0.00388]\n",
      "[train] ep 9, step 3514, accu 0.86719, loss 0.40194 [elapsed 0.00814]\n",
      "[train] ep 9, step 3765, accu 0.83594, loss 0.42600 [elapsed 0.00838]\n",
      " [test] ep 9, step 3862, accu 0.87640 [elapsed 0.00383]\n",
      "[train] ep 10, step 4016, accu 0.82812, loss 0.48153 [elapsed 0.00824]\n",
      "[train] ep 10, step 4267, accu 0.89844, loss 0.36563 [elapsed 0.00876]\n",
      " [test] ep 10, step 4291, accu 0.88361 [elapsed 0.00404]\n",
      "start training: 31/0.0/0.0/True/False/True\n",
      "('my_phi_initializer', 'inputs.shape', [128, 28])\n",
      "('my_phi', 'layer.shape', [128, 31])\n",
      "[train] ep 1, step 251, accu 0.46094, loss 1.57626 [elapsed 0.04584]\n",
      " [test] ep 1, step 430, accu 0.60417 [elapsed 0.01879]\n",
      "[train] ep 2, step 502, accu 0.56250, loss 1.23947 [elapsed 0.05023]\n",
      "[train] ep 2, step 753, accu 0.68750, loss 0.99212 [elapsed 0.04821]\n",
      " [test] ep 2, step 859, accu 0.72686 [elapsed 0.01775]\n",
      "[train] ep 3, step 1004, accu 0.79688, loss 0.77266 [elapsed 0.04942]\n",
      "[train] ep 3, step 1255, accu 0.80469, loss 0.65999 [elapsed 0.04803]\n",
      " [test] ep 3, step 1288, accu 0.77694 [elapsed 0.01720]\n",
      "[train] ep 4, step 1506, accu 0.82812, loss 0.59861 [elapsed 0.05134]\n",
      " [test] ep 4, step 1717, accu 0.83323 [elapsed 0.01702]\n",
      "[train] ep 5, step 1757, accu 0.82812, loss 0.58283 [elapsed 0.05019]\n",
      "[train] ep 5, step 2008, accu 0.85156, loss 0.52085 [elapsed 0.04684]\n",
      " [test] ep 5, step 2146, accu 0.87290 [elapsed 0.01715]\n",
      "[train] ep 6, step 2259, accu 0.89844, loss 0.40506 [elapsed 0.05141]\n",
      "[train] ep 6, step 2510, accu 0.94531, loss 0.26783 [elapsed 0.04961]\n",
      " [test] ep 6, step 2575, accu 0.89193 [elapsed 0.01696]\n",
      "[train] ep 7, step 2761, accu 0.85938, loss 0.43405 [elapsed 0.12423]\n",
      " [test] ep 7, step 3004, accu 0.90685 [elapsed 0.01693]\n",
      "[train] ep 8, step 3012, accu 0.92188, loss 0.37765 [elapsed 0.05588]\n",
      "[train] ep 8, step 3263, accu 0.90625, loss 0.32153 [elapsed 0.04716]\n",
      " [test] ep 8, step 3433, accu 0.91587 [elapsed 0.01731]\n",
      "[train] ep 9, step 3514, accu 0.92969, loss 0.28555 [elapsed 0.04930]\n",
      "[train] ep 9, step 3765, accu 0.87500, loss 0.34246 [elapsed 0.04696]\n",
      " [test] ep 9, step 3862, accu 0.92358 [elapsed 0.02010]\n",
      "[train] ep 10, step 4016, accu 0.89844, loss 0.36988 [elapsed 0.04843]\n",
      "[train] ep 10, step 4267, accu 0.95312, loss 0.17851 [elapsed 0.05016]\n",
      " [test] ep 10, step 4291, accu 0.92949 [elapsed 0.01700]\n",
      "start training: 15/0.0/0.0/True/False/True\n",
      "('my_phi_initializer', 'inputs.shape', [128, 28])\n",
      "('my_phi', 'layer.shape', [128, 15])\n",
      "[train] ep 1, step 251, accu 0.28906, loss 1.74988 [elapsed 0.03186]\n",
      " [test] ep 1, step 430, accu 0.45052 [elapsed 0.00996]\n",
      "[train] ep 2, step 502, accu 0.43750, loss 1.59205 [elapsed 0.02456]\n",
      "[train] ep 2, step 753, accu 0.49219, loss 1.43083 [elapsed 0.02501]\n",
      " [test] ep 2, step 859, accu 0.56210 [elapsed 0.00956]\n",
      "[train] ep 3, step 1004, accu 0.65625, loss 1.20089 [elapsed 0.02437]\n",
      "[train] ep 3, step 1255, accu 0.55469, loss 1.19278 [elapsed 0.02391]\n",
      " [test] ep 3, step 1288, accu 0.63161 [elapsed 0.01494]\n",
      "[train] ep 4, step 1506, accu 0.72656, loss 1.05784 [elapsed 0.02345]\n",
      " [test] ep 4, step 1717, accu 0.66707 [elapsed 0.00999]\n",
      "[train] ep 5, step 1757, accu 0.68750, loss 1.04149 [elapsed 0.02492]\n",
      "[train] ep 5, step 2008, accu 0.67188, loss 1.01256 [elapsed 0.02696]\n",
      " [test] ep 5, step 2146, accu 0.69081 [elapsed 0.00954]\n",
      "[train] ep 6, step 2259, accu 0.73438, loss 0.85159 [elapsed 0.02415]\n",
      "[train] ep 6, step 2510, accu 0.77344, loss 0.78324 [elapsed 0.02406]\n",
      " [test] ep 6, step 2575, accu 0.71194 [elapsed 0.00979]\n",
      "[train] ep 7, step 2761, accu 0.72656, loss 0.78953 [elapsed 0.02273]\n",
      " [test] ep 7, step 3004, accu 0.73237 [elapsed 0.00990]\n",
      "[train] ep 8, step 3012, accu 0.73438, loss 0.96054 [elapsed 0.02486]\n",
      "[train] ep 8, step 3263, accu 0.78125, loss 0.71402 [elapsed 0.02466]\n",
      " [test] ep 8, step 3433, accu 0.75431 [elapsed 0.01675]\n",
      "[train] ep 9, step 3514, accu 0.76562, loss 0.74719 [elapsed 0.02316]\n",
      "[train] ep 9, step 3765, accu 0.78125, loss 0.79771 [elapsed 0.03280]\n",
      " [test] ep 9, step 3862, accu 0.77584 [elapsed 0.00952]\n",
      "[train] ep 10, step 4016, accu 0.75000, loss 0.69234 [elapsed 0.02382]\n",
      "[train] ep 10, step 4267, accu 0.78125, loss 0.57562 [elapsed 0.02271]\n",
      " [test] ep 10, step 4291, accu 0.79788 [elapsed 0.00959]\n",
      "start training: 61/0.0/0.0/False/False/True\n",
      "[train] ep 1, step 251, accu 0.69531, loss 1.05007 [elapsed 0.02047]\n",
      " [test] ep 1, step 430, accu 0.82963 [elapsed 0.00992]\n",
      "[train] ep 2, step 502, accu 0.83594, loss 0.59943 [elapsed 0.02173]\n",
      "[train] ep 2, step 753, accu 0.82812, loss 0.50909 [elapsed 0.02214]\n",
      " [test] ep 2, step 859, accu 0.90304 [elapsed 0.00936]\n",
      "[train] ep 3, step 1004, accu 0.90625, loss 0.25872 [elapsed 0.02202]\n",
      "[train] ep 3, step 1255, accu 0.94531, loss 0.16343 [elapsed 0.02156]\n",
      " [test] ep 3, step 1288, accu 0.93309 [elapsed 0.00948]\n",
      "[train] ep 4, step 1506, accu 0.94531, loss 0.16831 [elapsed 0.02437]\n",
      " [test] ep 4, step 1717, accu 0.94842 [elapsed 0.00954]\n",
      "[train] ep 5, step 1757, accu 0.94531, loss 0.17629 [elapsed 0.03040]\n",
      "[train] ep 5, step 2008, accu 0.93750, loss 0.17005 [elapsed 0.02239]\n",
      " [test] ep 5, step 2146, accu 0.95803 [elapsed 0.00942]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] ep 6, step 2259, accu 0.94531, loss 0.13843 [elapsed 0.02203]\n",
      "[train] ep 6, step 2510, accu 0.98438, loss 0.06984 [elapsed 0.02087]\n",
      " [test] ep 6, step 2575, accu 0.96394 [elapsed 0.00943]\n",
      "[train] ep 7, step 2761, accu 0.96094, loss 0.10963 [elapsed 0.02228]\n",
      " [test] ep 7, step 3004, accu 0.96785 [elapsed 0.00940]\n",
      "[train] ep 8, step 3012, accu 0.97656, loss 0.09236 [elapsed 0.02183]\n",
      "[train] ep 8, step 3263, accu 0.96875, loss 0.08546 [elapsed 0.02207]\n",
      " [test] ep 8, step 3433, accu 0.97095 [elapsed 0.00968]\n",
      "[train] ep 9, step 3514, accu 0.99219, loss 0.05686 [elapsed 0.02246]\n",
      "[train] ep 9, step 3765, accu 0.95312, loss 0.09064 [elapsed 0.02164]\n",
      " [test] ep 9, step 3862, accu 0.97165 [elapsed 0.00944]\n",
      "[train] ep 10, step 4016, accu 0.95312, loss 0.11401 [elapsed 0.02171]\n",
      "[train] ep 10, step 4267, accu 0.98438, loss 0.08837 [elapsed 0.02202]\n",
      " [test] ep 10, step 4291, accu 0.97336 [elapsed 0.00943]\n",
      "start training: 61/0.0/0.0/True/False/True\n",
      "('my_phi_initializer', 'inputs.shape', [128, 28])\n",
      "('my_phi', 'layer.shape', [128, 61])\n",
      "[train] ep 1, step 251, accu 0.60156, loss 1.07762 [elapsed 0.11223]\n",
      " [test] ep 1, step 430, accu 0.80459 [elapsed 0.03399]\n",
      "[train] ep 2, step 502, accu 0.76562, loss 0.69281 [elapsed 0.11250]\n",
      "[train] ep 2, step 753, accu 0.84375, loss 0.51110 [elapsed 0.11068]\n",
      " [test] ep 2, step 859, accu 0.89694 [elapsed 0.03411]\n",
      "[train] ep 3, step 1004, accu 0.91406, loss 0.26365 [elapsed 0.11153]\n",
      "[train] ep 3, step 1255, accu 0.95312, loss 0.18505 [elapsed 0.15685]\n",
      " [test] ep 3, step 1288, accu 0.93049 [elapsed 0.04042]\n",
      "[train] ep 4, step 1506, accu 0.96094, loss 0.16808 [elapsed 0.11472]\n",
      " [test] ep 4, step 1717, accu 0.94591 [elapsed 0.03574]\n",
      "[train] ep 5, step 1757, accu 0.91406, loss 0.24316 [elapsed 0.11127]\n",
      "[train] ep 5, step 2008, accu 0.94531, loss 0.20174 [elapsed 0.11284]\n",
      " [test] ep 5, step 2146, accu 0.95363 [elapsed 0.03372]\n",
      "[train] ep 6, step 2259, accu 0.91406, loss 0.20261 [elapsed 0.11367]\n",
      "[train] ep 6, step 2510, accu 0.98438, loss 0.07670 [elapsed 0.10959]\n",
      " [test] ep 6, step 2575, accu 0.95923 [elapsed 0.03382]\n",
      "[train] ep 7, step 2761, accu 0.92188, loss 0.20781 [elapsed 0.11275]\n",
      " [test] ep 7, step 3004, accu 0.96434 [elapsed 0.03482]\n",
      "[train] ep 8, step 3012, accu 0.94531, loss 0.17053 [elapsed 0.11439]\n",
      "[train] ep 8, step 3263, accu 0.97656, loss 0.07821 [elapsed 0.14032]\n",
      " [test] ep 8, step 3433, accu 0.96635 [elapsed 0.04048]\n",
      "[train] ep 9, step 3514, accu 0.96094, loss 0.10892 [elapsed 0.12068]\n",
      "[train] ep 9, step 3765, accu 0.95312, loss 0.15722 [elapsed 0.12116]\n",
      " [test] ep 9, step 3862, accu 0.96955 [elapsed 0.04258]\n",
      "[train] ep 10, step 4016, accu 0.93750, loss 0.21041 [elapsed 0.11685]\n",
      "[train] ep 10, step 4267, accu 0.97656, loss 0.06583 [elapsed 0.12186]\n",
      " [test] ep 10, step 4291, accu 0.97326 [elapsed 0.04158]\n",
      "start training: 91/0.0/0.0/False/False/True\n",
      "[train] ep 1, step 251, accu 0.77344, loss 0.90617 [elapsed 0.03483]\n",
      " [test] ep 1, step 430, accu 0.85787 [elapsed 0.01342]\n",
      "[train] ep 2, step 502, accu 0.86719, loss 0.52177 [elapsed 0.03315]\n",
      "[train] ep 2, step 753, accu 0.84375, loss 0.45389 [elapsed 0.03498]\n",
      " [test] ep 2, step 859, accu 0.92658 [elapsed 0.01306]\n",
      "[train] ep 3, step 1004, accu 0.94531, loss 0.16720 [elapsed 0.03078]\n",
      "[train] ep 3, step 1255, accu 0.98438, loss 0.09160 [elapsed 0.03180]\n",
      " [test] ep 3, step 1288, accu 0.94681 [elapsed 0.01282]\n",
      "[train] ep 4, step 1506, accu 0.95312, loss 0.16012 [elapsed 0.03318]\n",
      " [test] ep 4, step 1717, accu 0.95883 [elapsed 0.01294]\n",
      "[train] ep 5, step 1757, accu 0.96875, loss 0.14744 [elapsed 0.03450]\n",
      "[train] ep 5, step 2008, accu 0.96094, loss 0.10873 [elapsed 0.03587]\n",
      " [test] ep 5, step 2146, accu 0.96635 [elapsed 0.01276]\n",
      "[train] ep 6, step 2259, accu 0.95312, loss 0.12675 [elapsed 0.03642]\n",
      "[train] ep 6, step 2510, accu 0.99219, loss 0.03778 [elapsed 0.03163]\n",
      " [test] ep 6, step 2575, accu 0.97105 [elapsed 0.01425]\n",
      "[train] ep 7, step 2761, accu 0.97656, loss 0.07714 [elapsed 0.03084]\n",
      " [test] ep 7, step 3004, accu 0.97356 [elapsed 0.01277]\n",
      "[train] ep 8, step 3012, accu 0.98438, loss 0.06385 [elapsed 0.03105]\n",
      "[train] ep 8, step 3263, accu 0.99219, loss 0.03537 [elapsed 0.03305]\n",
      " [test] ep 8, step 3433, accu 0.97536 [elapsed 0.01332]\n",
      "[train] ep 9, step 3514, accu 0.96875, loss 0.07131 [elapsed 0.03233]\n",
      "[train] ep 9, step 3765, accu 0.96094, loss 0.07849 [elapsed 0.03075]\n",
      " [test] ep 9, step 3862, accu 0.97606 [elapsed 0.01355]\n",
      "[train] ep 10, step 4016, accu 0.95312, loss 0.09405 [elapsed 0.03597]\n",
      "[train] ep 10, step 4267, accu 0.98438, loss 0.04905 [elapsed 0.03338]\n",
      " [test] ep 10, step 4291, accu 0.97716 [elapsed 0.01273]\n",
      "start training: 91/0.0/0.0/True/False/True\n",
      "('my_phi_initializer', 'inputs.shape', [128, 28])\n",
      "('my_phi', 'layer.shape', [128, 91])\n",
      "[train] ep 1, step 251, accu 0.73438, loss 0.95260 [elapsed 0.21554]\n",
      " [test] ep 1, step 430, accu 0.84275 [elapsed 0.06531]\n",
      "[train] ep 2, step 502, accu 0.85156, loss 0.53585 [elapsed 0.20399]\n",
      "[train] ep 2, step 753, accu 0.86719, loss 0.40995 [elapsed 0.39056]\n",
      " [test] ep 2, step 859, accu 0.91767 [elapsed 0.06257]\n",
      "[train] ep 3, step 1004, accu 0.94531, loss 0.16829 [elapsed 0.21214]\n",
      "[train] ep 3, step 1255, accu 0.96875, loss 0.13465 [elapsed 0.21062]\n",
      " [test] ep 3, step 1288, accu 0.94141 [elapsed 0.06553]\n",
      "[train] ep 4, step 1506, accu 0.96875, loss 0.13226 [elapsed 0.20271]\n",
      " [test] ep 4, step 1717, accu 0.95242 [elapsed 0.06389]\n",
      "[train] ep 5, step 1757, accu 0.96875, loss 0.15181 [elapsed 0.19942]\n",
      "[train] ep 5, step 2008, accu 0.96094, loss 0.13423 [elapsed 0.21671]\n",
      " [test] ep 5, step 2146, accu 0.96444 [elapsed 0.06357]\n",
      "[train] ep 6, step 2259, accu 0.94531, loss 0.15010 [elapsed 0.20856]\n",
      "[train] ep 6, step 2510, accu 0.97656, loss 0.07028 [elapsed 0.21950]\n",
      " [test] ep 6, step 2575, accu 0.96705 [elapsed 0.07051]\n",
      "[train] ep 7, step 2761, accu 0.94531, loss 0.14255 [elapsed 0.25158]\n",
      " [test] ep 7, step 3004, accu 0.97075 [elapsed 0.06341]\n",
      "[train] ep 8, step 3012, accu 0.96094, loss 0.12723 [elapsed 0.20215]\n",
      "[train] ep 8, step 3263, accu 0.97656, loss 0.07426 [elapsed 0.21693]\n",
      " [test] ep 8, step 3433, accu 0.97496 [elapsed 0.06373]\n",
      "[train] ep 9, step 3514, accu 0.96094, loss 0.11226 [elapsed 0.21897]\n",
      "[train] ep 9, step 3765, accu 0.96094, loss 0.12530 [elapsed 0.21265]\n",
      " [test] ep 9, step 3862, accu 0.97416 [elapsed 0.06301]\n",
      "[train] ep 10, step 4016, accu 0.93750, loss 0.20594 [elapsed 0.20615]\n",
      "[train] ep 10, step 4267, accu 0.98438, loss 0.05750 [elapsed 0.23050]\n",
      " [test] ep 10, step 4291, accu 0.97646 [elapsed 0.06393]\n"
     ]
    }
   ],
   "source": [
    "for args in hp_configs:\n",
    "\n",
    "    signature = hp_signature(args)\n",
    "    print('start training:',signature)\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    inputs_      = tf.placeholder(tf.float32,\n",
    "                             [BATCH_SIZE, MAX_SEQ_LEN, INPUT_UNITS],\n",
    "                             name='inputs')\n",
    "    labels_      = tf.placeholder(tf.int64,\n",
    "                             [BATCH_SIZE],\n",
    "                             name='labels')\n",
    "    model        = MnistRnn(args, inputs_, labels_)\n",
    "\n",
    "    sess         = tf.Session(config=tf.ConfigProto(gpu_options={'allow_growth':True}))\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    train_writer = tf.summary.FileWriter('logdir/train_{:s}'.format(signature))\n",
    "    test_writer  = tf.summary.FileWriter('logdir/test__{:s}'.format(signature))\n",
    "\n",
    "    train(args, sess, model, 10, train_writer, test_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:34:29.412212Z",
     "start_time": "2017-12-12T13:34:28.385547Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mE1219 15:21:35.826254 MainThread program.py:255] TensorBoard attempted to bind to port 6006, but it was already in use\r\n",
      "\u001b[0mTensorBoard attempted to bind to port 6006, but it was already in use\r\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir logdir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
