{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:24:05.253664Z",
     "start_time": "2017-12-12T13:24:04.493518Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "from __future__ import print_function, division, absolute_import\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:24:05.372975Z",
     "start_time": "2017-12-12T13:24:05.255061Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -fr logdir\n",
    "!mkdir -p logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:24:05.811621Z",
     "start_time": "2017-12-12T13:24:05.506894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist.input_data \\\n",
    "  import read_data_sets\n",
    "\n",
    "mnist = read_data_sets('./mnist', one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:24:06.131307Z",
     "start_time": "2017-12-12T13:24:06.128673Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_UNITS = 28\n",
    "NUM_HIDDEN_UNITS = 31\n",
    "BATCH_SIZE = 128\n",
    "MAX_SEQ_LEN = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:24:06.139750Z",
     "start_time": "2017-12-12T13:24:06.132935Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loop_count = mnist.train.num_examples // BATCH_SIZE\n",
    "test_loop_count  = mnist.test.num_examples // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:24:06.148736Z",
     "start_time": "2017-12-12T13:24:06.141258Z"
    }
   },
   "outputs": [],
   "source": [
    "from minimalrnn import MinimalRNNCell\n",
    "from tensorflow.python.layers.layers import conv1d,max_pooling1d,dropout,batch_normalization\n",
    "\n",
    "class MnistRnn:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, inputs, labels,\n",
    "                 use_custom_phi=False,\n",
    "                 use_batch_norm=False,\n",
    "                 use_rnn_dropout=False):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        training    = tf.Variable(False,name='training',dtype=tf.bool)\n",
    "        global_step = tf.Variable(1,name='global_step',dtype=tf.int64)\n",
    "        dropout_rate = tf.Variable(0.0,name='dropout_rate',dtype=tf.float32)\n",
    "\n",
    "        keep_prob   = tf.cond(training,lambda: 1.0 - dropout_rate,lambda: 1.0)\n",
    "\n",
    "        step_update = tf.placeholder(tf.int64,None,name='step_update')\n",
    "        dropout_update = tf.placeholder(tf.float32,None,name='dropout_update')\n",
    "\n",
    "        def my_phi_initializer(inputs, num_outputs, **kwargs):\n",
    "            \n",
    "            print(('my_phi_initializer','inputs.shape',inputs.get_shape().as_list()))\n",
    "            \n",
    "            def my_phi(inputs):\n",
    "                \n",
    "                input_units = inputs.get_shape().as_list()[-1]\n",
    "                layer   = tf.reshape(inputs, [-1,input_units,1])\n",
    "\n",
    "                layer   = conv1d(layer, num_outputs//3, 5, 1, activation=tf.nn.relu)\n",
    "                layer   = max_pooling1d(layer, 3, 2)\n",
    "                layer   = conv1d(layer, num_outputs, 3, 1, activation=tf.nn.relu)\n",
    "                layer   = conv1d(layer, num_outputs, 3, 1, activation=tf.nn.relu)\n",
    "                layer   = conv1d(layer, num_outputs//2, 3, 1, activation=tf.nn.relu)\n",
    "                layer   = max_pooling1d(layer, 3, 2)\n",
    "\n",
    "                layer   = conv1d(layer, num_outputs, 1, 1, activation=tf.nn.relu)\n",
    "                layer   = dropout(layer, rate=1.0-keep_prob)\n",
    "                layer   = conv1d(layer, num_outputs, 1, 1, activation=tf.nn.relu)\n",
    "\n",
    "                layer   = tf.reduce_mean(layer, axis=1, keep_dims=True)\n",
    "                layer   = tf.squeeze(layer)\n",
    "\n",
    "                print(('my_phi','output.shape',layer.get_shape().as_list()))\n",
    "                \n",
    "                return layer\n",
    "            \n",
    "            \n",
    "            return my_phi\n",
    "\n",
    "\n",
    "        cell = MinimalRNNCell(NUM_HIDDEN_UNITS,\n",
    "                              phi_initializer=my_phi_initializer if use_custom_phi else None)\n",
    "\n",
    "        if use_rnn_dropout:\n",
    "            cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "                cell,\n",
    "                state_keep_prob=keep_prob,\n",
    "                variational_recurrent=True,\n",
    "                dtype=tf.float32\n",
    "            )\n",
    "        \n",
    "        sequence_length = [MAX_SEQ_LEN] * BATCH_SIZE\n",
    "        \n",
    "        last, states = tf.nn.dynamic_rnn(\n",
    "            cell,\n",
    "            inputs,\n",
    "            sequence_length=sequence_length,\n",
    "            dtype=tf.float32)\n",
    "        rnn_output = last[:,MAX_SEQ_LEN-1,:]\n",
    "        outputs    = tf.layers.dense(rnn_output, 10)\n",
    "        \n",
    "        loss       = tf.losses.sparse_softmax_cross_entropy(\n",
    "            labels,\n",
    "            outputs)\n",
    "\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            optimize   = tf.train.AdamOptimizer(learning_rate=0.001). \\\n",
    "                minimize(loss)\n",
    "\n",
    "        preds      = tf.argmax(outputs, axis=1)\n",
    "        errors     = tf.count_nonzero(labels - preds)\n",
    "        accuracy   = 1.0 - tf.cast(errors,tf.float32) / \\\n",
    "                        tf.cast(tf.size(preds), tf.float32)\n",
    "        \n",
    "        with tf.control_dependencies([\n",
    "            tf.assign(training, True),\n",
    "            tf.assign(global_step, step_update),\n",
    "            tf.assign(dropout_rate, dropout_update)\n",
    "        ]):\n",
    "            self.train_mode = tf.constant(1)\n",
    "            \n",
    "        with tf.control_dependencies([\n",
    "            tf.assign(training, False)\n",
    "        ]):\n",
    "            self.test_mode  = tf.constant(1)\n",
    "\n",
    "        self.step_update = step_update\n",
    "        self.dropout_update = dropout_update\n",
    "        self.inputs   = inputs\n",
    "        self.labels   = labels\n",
    "        self.outputs  = outputs\n",
    "        self.loss     = loss\n",
    "        self.optimize = optimize\n",
    "        self.accuracy = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:24:06.261738Z",
     "start_time": "2017-12-12T13:24:06.236746Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(sess, model, max_epochs, train_writer=None, test_writer=None, dropout_rate=0.5):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    step = sess.run(tf.train.get_global_step())\n",
    "\n",
    "    for ep in range(max_epochs):\n",
    "\n",
    "        for i in range(train_loop_count):\n",
    "\n",
    "            sess.run(model.train_mode, {\n",
    "                model.step_update: step,\n",
    "                model.dropout_update: dropout_rate})\n",
    "\n",
    "            offs = i * BATCH_SIZE\n",
    "            batch_input = mnist.train.images[offs:offs+BATCH_SIZE,:]. \\\n",
    "                            reshape([BATCH_SIZE, MAX_SEQ_LEN, INPUT_UNITS])\n",
    "            batch_label = mnist.train.labels[offs:offs+BATCH_SIZE]\n",
    "            t_start     = time.time()\n",
    "            _, loss, accuracy = sess.run(\n",
    "                [model.optimize, model.loss, model.accuracy],\n",
    "                 feed_dict = {\n",
    "                     model.inputs: batch_input,\n",
    "                     model.labels: batch_label })\n",
    "            t_elapsed     = time.time() - t_start\n",
    "            step += 1\n",
    "            \n",
    "            if train_writer:\n",
    "                summary = tf.Summary(\n",
    "                    value = [\n",
    "                        tf.Summary.Value(tag='accuracy',simple_value=accuracy),\n",
    "                        tf.Summary.Value(tag='loss',simple_value=loss),\n",
    "                        tf.Summary.Value(tag='elapsed',simple_value=t_elapsed),\n",
    "                    ])\n",
    "                train_writer.add_summary(summary, global_step=step)\n",
    "                train_writer.flush()\n",
    "                \n",
    "            if step % 251 == 0:\n",
    "                print('[train] ep {:d}, step {:d}, accu {:.5f}, loss {:.5f} [elapsed {:.5f}]'.format(\n",
    "                    ep + 1, step, accuracy, loss, t_elapsed))\n",
    "                \n",
    "        test_accuracies = []\n",
    "        \n",
    "        sess.run(model.test_mode)\n",
    "\n",
    "        for i in range(test_loop_count):\n",
    "            offs = i * BATCH_SIZE\n",
    "            batch_input = mnist.test.images[offs:offs+BATCH_SIZE,:]. \\\n",
    "                            reshape([BATCH_SIZE, MAX_SEQ_LEN, INPUT_UNITS])\n",
    "            batch_label = mnist.test.labels[offs:offs+BATCH_SIZE]\n",
    "            t_start   = time.time()\n",
    "            accuracy, = sess.run([model.accuracy],\n",
    "                                 feed_dict = {\n",
    "                                     model.inputs: batch_input,\n",
    "                                     model.labels: batch_label})\n",
    "            t_elapsed = time.time() - t_start\n",
    "            test_accuracies.append(accuracy)\n",
    "            \n",
    "            if test_writer:\n",
    "                summary = tf.Summary(\n",
    "                    value = [\n",
    "                        tf.Summary.Value(tag='accuracy',simple_value=accuracy),\n",
    "                        tf.Summary.Value(tag='elapsed',simple_value=t_elapsed),\n",
    "                    ])\n",
    "                test_writer.add_summary(summary, global_step=step)\n",
    "                test_writer.flush()\n",
    "                \n",
    "            if i % 503 == 0:\n",
    "                print(' [test] ep {:d}, step {:d}, accu {:.5f} [elapsed {:.5f}]'.format(\n",
    "                    ep + 1, step, np.mean(test_accuracies), t_elapsed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:24:06.817908Z",
     "start_time": "2017-12-12T13:24:06.264586Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "inputs_ = tf.placeholder(tf.float32,\n",
    "                         [BATCH_SIZE, MAX_SEQ_LEN, INPUT_UNITS],\n",
    "                         name='inputs')\n",
    "labels_ = tf.placeholder(tf.int64,\n",
    "                         [BATCH_SIZE],\n",
    "                         name='labels')\n",
    "model   = MnistRnn(inputs_, labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(gpu_options={'allow_growth':True})\n",
    "sess   = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "train_writer = tf.summary.FileWriter('logdir/train1')\n",
    "test_writer  = tf.summary.FileWriter('logdir/test1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:25:00.230356Z",
     "start_time": "2017-12-12T13:24:08.105917Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] ep 1, step 251, accu 0.54688, loss 1.53519 [elapsed 0.01117]\n",
      " [test] ep 1, step 430, accu 0.68750 [elapsed 0.00944]\n",
      "[train] ep 2, step 502, accu 0.69531, loss 0.94320 [elapsed 0.00977]\n",
      "[train] ep 2, step 753, accu 0.79688, loss 0.63411 [elapsed 0.00990]\n",
      " [test] ep 2, step 859, accu 0.83594 [elapsed 0.00543]\n",
      "[train] ep 3, step 1004, accu 0.83594, loss 0.45320 [elapsed 0.01252]\n",
      "[train] ep 3, step 1255, accu 0.90625, loss 0.31008 [elapsed 0.01066]\n",
      " [test] ep 3, step 1288, accu 0.87500 [elapsed 0.00417]\n",
      "[train] ep 4, step 1506, accu 0.88281, loss 0.38491 [elapsed 0.00973]\n",
      " [test] ep 4, step 1717, accu 0.89062 [elapsed 0.00448]\n",
      "[train] ep 5, step 1757, accu 0.92188, loss 0.32144 [elapsed 0.00967]\n",
      "[train] ep 5, step 2008, accu 0.86719, loss 0.36720 [elapsed 0.00947]\n",
      " [test] ep 5, step 2146, accu 0.92969 [elapsed 0.00542]\n",
      "[train] ep 6, step 2259, accu 0.92188, loss 0.25181 [elapsed 0.00998]\n",
      "[train] ep 6, step 2510, accu 0.96875, loss 0.14967 [elapsed 0.00961]\n",
      " [test] ep 6, step 2575, accu 0.92969 [elapsed 0.00515]\n",
      "[train] ep 7, step 2761, accu 0.96875, loss 0.13714 [elapsed 0.00992]\n",
      " [test] ep 7, step 3004, accu 0.93750 [elapsed 0.00559]\n",
      "[train] ep 8, step 3012, accu 0.94531, loss 0.18132 [elapsed 0.01050]\n",
      "[train] ep 8, step 3263, accu 0.94531, loss 0.17684 [elapsed 0.00971]\n",
      " [test] ep 8, step 3433, accu 0.96094 [elapsed 0.00462]\n",
      "[train] ep 9, step 3514, accu 0.92188, loss 0.23885 [elapsed 0.00987]\n",
      "[train] ep 9, step 3765, accu 0.92969, loss 0.20844 [elapsed 0.01027]\n",
      " [test] ep 9, step 3862, accu 0.96094 [elapsed 0.00454]\n",
      "[train] ep 10, step 4016, accu 0.89844, loss 0.22917 [elapsed 0.01013]\n",
      "[train] ep 10, step 4267, accu 0.96094, loss 0.15018 [elapsed 0.00968]\n",
      " [test] ep 10, step 4291, accu 0.96875 [elapsed 0.00492]\n",
      "[train] ep 11, step 4518, accu 0.95312, loss 0.13713 [elapsed 0.01211]\n",
      " [test] ep 11, step 4720, accu 0.96875 [elapsed 0.00422]\n",
      "[train] ep 12, step 4769, accu 0.91406, loss 0.29954 [elapsed 0.01060]\n",
      "[train] ep 12, step 5020, accu 0.96875, loss 0.06951 [elapsed 0.00995]\n",
      " [test] ep 12, step 5149, accu 0.97656 [elapsed 0.00420]\n",
      "[train] ep 13, step 5271, accu 0.94531, loss 0.13710 [elapsed 0.01204]\n",
      "[train] ep 13, step 5522, accu 0.94531, loss 0.17070 [elapsed 0.00980]\n",
      " [test] ep 13, step 5578, accu 0.97656 [elapsed 0.00450]\n",
      "[train] ep 14, step 5773, accu 0.94531, loss 0.18201 [elapsed 0.01013]\n",
      " [test] ep 14, step 6007, accu 0.97656 [elapsed 0.00485]\n",
      "[train] ep 15, step 6024, accu 0.93750, loss 0.16215 [elapsed 0.00970]\n",
      "[train] ep 15, step 6275, accu 0.94531, loss 0.16028 [elapsed 0.00973]\n",
      " [test] ep 15, step 6436, accu 0.97656 [elapsed 0.00419]\n",
      "[train] ep 16, step 6526, accu 0.97656, loss 0.09990 [elapsed 0.00990]\n",
      "[train] ep 16, step 6777, accu 0.96875, loss 0.11067 [elapsed 0.01015]\n",
      " [test] ep 16, step 6865, accu 0.96875 [elapsed 0.00424]\n",
      "[train] ep 17, step 7028, accu 0.95312, loss 0.19480 [elapsed 0.01098]\n",
      "[train] ep 17, step 7279, accu 0.96094, loss 0.08476 [elapsed 0.01179]\n",
      " [test] ep 17, step 7294, accu 0.96875 [elapsed 0.00500]\n",
      "[train] ep 18, step 7530, accu 0.96094, loss 0.09685 [elapsed 0.01076]\n",
      " [test] ep 18, step 7723, accu 0.96875 [elapsed 0.00417]\n",
      "[train] ep 19, step 7781, accu 0.96094, loss 0.11704 [elapsed 0.00960]\n",
      "[train] ep 19, step 8032, accu 0.95312, loss 0.12901 [elapsed 0.01138]\n",
      " [test] ep 19, step 8152, accu 0.96875 [elapsed 0.00493]\n",
      "[train] ep 20, step 8283, accu 0.97656, loss 0.05448 [elapsed 0.00940]\n",
      "[train] ep 20, step 8534, accu 0.94531, loss 0.19237 [elapsed 0.01023]\n",
      " [test] ep 20, step 8581, accu 0.96875 [elapsed 0.00471]\n",
      "[train] ep 21, step 8785, accu 0.97656, loss 0.09501 [elapsed 0.00914]\n",
      " [test] ep 21, step 9010, accu 0.96875 [elapsed 0.00574]\n",
      "[train] ep 22, step 9036, accu 0.94531, loss 0.23908 [elapsed 0.00987]\n",
      "[train] ep 22, step 9287, accu 0.96875, loss 0.08805 [elapsed 0.01354]\n",
      " [test] ep 22, step 9439, accu 0.96875 [elapsed 0.00458]\n",
      "[train] ep 23, step 9538, accu 0.94531, loss 0.11746 [elapsed 0.01040]\n",
      "[train] ep 23, step 9789, accu 0.97656, loss 0.06168 [elapsed 0.00993]\n",
      " [test] ep 23, step 9868, accu 0.96875 [elapsed 0.00472]\n",
      "[train] ep 24, step 10040, accu 0.98438, loss 0.04600 [elapsed 0.01112]\n",
      "[train] ep 24, step 10291, accu 1.00000, loss 0.01219 [elapsed 0.00976]\n",
      " [test] ep 24, step 10297, accu 0.96875 [elapsed 0.00490]\n",
      "[train] ep 25, step 10542, accu 0.97656, loss 0.08642 [elapsed 0.00979]\n",
      " [test] ep 25, step 10726, accu 0.96875 [elapsed 0.00449]\n",
      "[train] ep 26, step 10793, accu 0.98438, loss 0.07575 [elapsed 0.01093]\n",
      "[train] ep 26, step 11044, accu 0.95312, loss 0.09730 [elapsed 0.00972]\n",
      " [test] ep 26, step 11155, accu 0.96875 [elapsed 0.00466]\n",
      "[train] ep 27, step 11295, accu 0.97656, loss 0.06318 [elapsed 0.00962]\n",
      "[train] ep 27, step 11546, accu 0.97656, loss 0.11545 [elapsed 0.00976]\n",
      " [test] ep 27, step 11584, accu 0.97656 [elapsed 0.00520]\n",
      "[train] ep 28, step 11797, accu 0.96875, loss 0.08195 [elapsed 0.00930]\n",
      " [test] ep 28, step 12013, accu 0.97656 [elapsed 0.00482]\n",
      "[train] ep 29, step 12048, accu 0.97656, loss 0.10121 [elapsed 0.00957]\n",
      "[train] ep 29, step 12299, accu 0.97656, loss 0.09626 [elapsed 0.01015]\n",
      " [test] ep 29, step 12442, accu 0.97656 [elapsed 0.00519]\n",
      "[train] ep 30, step 12550, accu 0.97656, loss 0.06704 [elapsed 0.01091]\n",
      "[train] ep 30, step 12801, accu 0.95312, loss 0.13192 [elapsed 0.00968]\n",
      " [test] ep 30, step 12871, accu 0.97656 [elapsed 0.00480]\n",
      "[train] ep 31, step 13052, accu 0.98438, loss 0.04151 [elapsed 0.01823]\n",
      " [test] ep 31, step 13300, accu 0.97656 [elapsed 0.00477]\n",
      "[train] ep 32, step 13303, accu 0.96875, loss 0.11018 [elapsed 0.00973]\n",
      "[train] ep 32, step 13554, accu 0.98438, loss 0.10983 [elapsed 0.00970]\n",
      " [test] ep 32, step 13729, accu 0.97656 [elapsed 0.00501]\n",
      "[train] ep 33, step 13805, accu 0.94531, loss 0.18840 [elapsed 0.00988]\n",
      "[train] ep 33, step 14056, accu 1.00000, loss 0.02707 [elapsed 0.01022]\n",
      " [test] ep 33, step 14158, accu 0.97656 [elapsed 0.00490]\n",
      "[train] ep 34, step 14307, accu 0.96875, loss 0.08217 [elapsed 0.01140]\n",
      "[train] ep 34, step 14558, accu 0.98438, loss 0.05383 [elapsed 0.00960]\n",
      " [test] ep 34, step 14587, accu 0.97656 [elapsed 0.00458]\n",
      "[train] ep 35, step 14809, accu 0.98438, loss 0.07979 [elapsed 0.01189]\n",
      " [test] ep 35, step 15016, accu 0.97656 [elapsed 0.00433]\n",
      "[train] ep 36, step 15060, accu 0.99219, loss 0.02883 [elapsed 0.00927]\n",
      "[train] ep 36, step 15311, accu 0.96094, loss 0.08346 [elapsed 0.01061]\n",
      " [test] ep 36, step 15445, accu 0.97656 [elapsed 0.00421]\n",
      "[train] ep 37, step 15562, accu 0.96875, loss 0.06249 [elapsed 0.00963]\n",
      "[train] ep 37, step 15813, accu 0.98438, loss 0.06875 [elapsed 0.00960]\n",
      " [test] ep 37, step 15874, accu 0.97656 [elapsed 0.00496]\n",
      "[train] ep 38, step 16064, accu 0.95312, loss 0.15279 [elapsed 0.01028]\n",
      " [test] ep 38, step 16303, accu 0.97656 [elapsed 0.00490]\n",
      "[train] ep 39, step 16315, accu 0.98438, loss 0.05957 [elapsed 0.01001]\n",
      "[train] ep 39, step 16566, accu 0.99219, loss 0.07242 [elapsed 0.01017]\n",
      " [test] ep 39, step 16732, accu 0.97656 [elapsed 0.00445]\n",
      "[train] ep 40, step 16817, accu 0.96875, loss 0.11693 [elapsed 0.00975]\n",
      "[train] ep 40, step 17068, accu 0.98438, loss 0.06896 [elapsed 0.01058]\n",
      " [test] ep 40, step 17161, accu 0.97656 [elapsed 0.00480]\n",
      "[train] ep 41, step 17319, accu 0.99219, loss 0.02828 [elapsed 0.00951]\n",
      "[train] ep 41, step 17570, accu 0.97656, loss 0.04918 [elapsed 0.00954]\n",
      " [test] ep 41, step 17590, accu 0.97656 [elapsed 0.00466]\n",
      "[train] ep 42, step 17821, accu 0.96875, loss 0.09060 [elapsed 0.01771]\n",
      " [test] ep 42, step 18019, accu 0.97656 [elapsed 0.00513]\n",
      "[train] ep 43, step 18072, accu 0.98438, loss 0.06325 [elapsed 0.01053]\n",
      "[train] ep 43, step 18323, accu 0.98438, loss 0.05549 [elapsed 0.01048]\n",
      " [test] ep 43, step 18448, accu 0.97656 [elapsed 0.00603]\n",
      "[train] ep 44, step 18574, accu 0.98438, loss 0.07604 [elapsed 0.00999]\n",
      "[train] ep 44, step 18825, accu 0.99219, loss 0.04675 [elapsed 0.00969]\n",
      " [test] ep 44, step 18877, accu 0.97656 [elapsed 0.00519]\n",
      "[train] ep 45, step 19076, accu 0.99219, loss 0.02689 [elapsed 0.01010]\n",
      " [test] ep 45, step 19306, accu 0.97656 [elapsed 0.00416]\n",
      "[train] ep 46, step 19327, accu 0.97656, loss 0.05337 [elapsed 0.00939]\n",
      "[train] ep 46, step 19578, accu 0.97656, loss 0.04596 [elapsed 0.01045]\n",
      " [test] ep 46, step 19735, accu 0.97656 [elapsed 0.00457]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] ep 47, step 19829, accu 0.97656, loss 0.10964 [elapsed 0.01080]\n",
      "[train] ep 47, step 20080, accu 0.96094, loss 0.17524 [elapsed 0.01129]\n",
      " [test] ep 47, step 20164, accu 0.97656 [elapsed 0.00444]\n",
      "[train] ep 48, step 20331, accu 0.97656, loss 0.09736 [elapsed 0.00930]\n",
      "[train] ep 48, step 20582, accu 0.96875, loss 0.08364 [elapsed 0.00932]\n",
      " [test] ep 48, step 20593, accu 0.97656 [elapsed 0.00522]\n",
      "[train] ep 49, step 20833, accu 0.99219, loss 0.02994 [elapsed 0.00954]\n",
      " [test] ep 49, step 21022, accu 0.97656 [elapsed 0.00485]\n",
      "[train] ep 50, step 21084, accu 0.98438, loss 0.06853 [elapsed 0.00978]\n",
      "[train] ep 50, step 21335, accu 0.98438, loss 0.05367 [elapsed 0.00996]\n",
      " [test] ep 50, step 21451, accu 0.97656 [elapsed 0.00422]\n"
     ]
    }
   ],
   "source": [
    "train(sess, model, 50, train_writer, test_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:25:02.018412Z",
     "start_time": "2017-12-12T13:25:00.231543Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('my_phi_initializer', 'inputs.shape', [128, 28])\n",
      "('my_phi', 'output.shape', [128, 31])\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "inputs2_ = tf.placeholder(tf.float32,\n",
    "                         [BATCH_SIZE, MAX_SEQ_LEN, INPUT_UNITS],\n",
    "                         name='inputs')\n",
    "labels2_ = tf.placeholder(tf.int64,\n",
    "                         [BATCH_SIZE],\n",
    "                         name='labels')\n",
    "model2   = MnistRnn(inputs2_, labels2_, use_custom_phi = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:25:02.021870Z",
     "start_time": "2017-12-12T13:25:02.019479Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(gpu_options={'allow_growth':True})\n",
    "sess   = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "train_writer2 = tf.summary.FileWriter('logdir/train2')\n",
    "test_writer2  = tf.summary.FileWriter('logdir/test2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:34:28.384362Z",
     "start_time": "2017-12-12T13:25:06.373902Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] ep 1, step 251, accu 0.54688, loss 1.52188 [elapsed 0.05732]\n",
      " [test] ep 1, step 430, accu 0.65625 [elapsed 0.02680]\n",
      "[train] ep 2, step 502, accu 0.69531, loss 1.14380 [elapsed 0.04807]\n",
      "[train] ep 2, step 753, accu 0.67188, loss 1.00010 [elapsed 0.04667]\n",
      " [test] ep 2, step 859, accu 0.74219 [elapsed 0.01850]\n",
      "[train] ep 3, step 1004, accu 0.82031, loss 0.63754 [elapsed 0.04778]\n",
      "[train] ep 3, step 1255, accu 0.85156, loss 0.56439 [elapsed 0.05032]\n",
      " [test] ep 3, step 1288, accu 0.78906 [elapsed 0.01817]\n",
      "[train] ep 4, step 1506, accu 0.80469, loss 0.53878 [elapsed 0.04853]\n",
      " [test] ep 4, step 1717, accu 0.85156 [elapsed 0.01544]\n",
      "[train] ep 5, step 1757, accu 0.79688, loss 0.52974 [elapsed 0.04974]\n",
      "[train] ep 5, step 2008, accu 0.87500, loss 0.45741 [elapsed 0.06600]\n",
      " [test] ep 5, step 2146, accu 0.89062 [elapsed 0.01638]\n",
      "[train] ep 6, step 2259, accu 0.87500, loss 0.40388 [elapsed 0.04743]\n",
      "[train] ep 6, step 2510, accu 0.92969, loss 0.29526 [elapsed 0.07458]\n",
      " [test] ep 6, step 2575, accu 0.91406 [elapsed 0.01888]\n",
      "[train] ep 7, step 2761, accu 0.92188, loss 0.32155 [elapsed 0.04917]\n",
      " [test] ep 7, step 3004, accu 0.91406 [elapsed 0.01747]\n",
      "[train] ep 8, step 3012, accu 0.90625, loss 0.42824 [elapsed 0.04465]\n",
      "[train] ep 8, step 3263, accu 0.89844, loss 0.26925 [elapsed 0.04873]\n",
      " [test] ep 8, step 3433, accu 0.93750 [elapsed 0.01954]\n",
      "[train] ep 9, step 3514, accu 0.91406, loss 0.35970 [elapsed 0.04380]\n",
      "[train] ep 9, step 3765, accu 0.91406, loss 0.30175 [elapsed 0.05118]\n",
      " [test] ep 9, step 3862, accu 0.94531 [elapsed 0.01494]\n",
      "[train] ep 10, step 4016, accu 0.85156, loss 0.44211 [elapsed 0.04515]\n",
      "[train] ep 10, step 4267, accu 0.93750, loss 0.23856 [elapsed 0.04713]\n",
      " [test] ep 10, step 4291, accu 0.96094 [elapsed 0.02109]\n",
      "[train] ep 11, step 4518, accu 0.94531, loss 0.28479 [elapsed 0.04330]\n",
      " [test] ep 11, step 4720, accu 0.96094 [elapsed 0.01801]\n",
      "[train] ep 12, step 4769, accu 0.89844, loss 0.30387 [elapsed 0.08990]\n",
      "[train] ep 12, step 5020, accu 0.99219, loss 0.05422 [elapsed 0.04787]\n",
      " [test] ep 12, step 5149, accu 0.98438 [elapsed 0.01636]\n",
      "[train] ep 13, step 5271, accu 0.96875, loss 0.11446 [elapsed 0.04837]\n",
      "[train] ep 13, step 5522, accu 0.95312, loss 0.16500 [elapsed 0.04989]\n",
      " [test] ep 13, step 5578, accu 0.98438 [elapsed 0.01751]\n",
      "[train] ep 14, step 5773, accu 0.88281, loss 0.38297 [elapsed 0.04988]\n",
      " [test] ep 14, step 6007, accu 0.97656 [elapsed 0.01880]\n",
      "[train] ep 15, step 6024, accu 0.92969, loss 0.20889 [elapsed 0.04951]\n",
      "[train] ep 15, step 6275, accu 0.94531, loss 0.18060 [elapsed 0.04743]\n",
      " [test] ep 15, step 6436, accu 0.97656 [elapsed 0.01772]\n",
      "[train] ep 16, step 6526, accu 0.96875, loss 0.18342 [elapsed 0.04670]\n",
      "[train] ep 16, step 6777, accu 0.96094, loss 0.13064 [elapsed 0.05059]\n",
      " [test] ep 16, step 6865, accu 0.97656 [elapsed 0.01550]\n",
      "[train] ep 17, step 7028, accu 0.95312, loss 0.15641 [elapsed 0.06635]\n",
      "[train] ep 17, step 7279, accu 0.97656, loss 0.07040 [elapsed 0.04928]\n",
      " [test] ep 17, step 7294, accu 0.97656 [elapsed 0.01616]\n",
      "[train] ep 18, step 7530, accu 0.98438, loss 0.07930 [elapsed 0.04917]\n",
      " [test] ep 18, step 7723, accu 0.98438 [elapsed 0.01593]\n",
      "[train] ep 19, step 7781, accu 0.91406, loss 0.20173 [elapsed 0.06052]\n",
      "[train] ep 19, step 8032, accu 0.96875, loss 0.10035 [elapsed 0.05869]\n",
      " [test] ep 19, step 8152, accu 0.98438 [elapsed 0.01932]\n",
      "[train] ep 20, step 8283, accu 0.97656, loss 0.06750 [elapsed 0.05074]\n",
      "[train] ep 20, step 8534, accu 0.95312, loss 0.13712 [elapsed 0.05538]\n",
      " [test] ep 20, step 8581, accu 0.98438 [elapsed 0.01984]\n",
      "[train] ep 21, step 8785, accu 0.99219, loss 0.07437 [elapsed 0.06097]\n",
      " [test] ep 21, step 9010, accu 0.98438 [elapsed 0.01944]\n",
      "[train] ep 22, step 9036, accu 0.95312, loss 0.18564 [elapsed 0.05764]\n",
      "[train] ep 22, step 9287, accu 0.98438, loss 0.07242 [elapsed 0.04549]\n",
      " [test] ep 22, step 9439, accu 0.98438 [elapsed 0.01626]\n",
      "[train] ep 23, step 9538, accu 0.96094, loss 0.16566 [elapsed 0.04930]\n",
      "[train] ep 23, step 9789, accu 0.96875, loss 0.08453 [elapsed 0.04588]\n",
      " [test] ep 23, step 9868, accu 0.99219 [elapsed 0.01803]\n",
      "[train] ep 24, step 10040, accu 0.99219, loss 0.03841 [elapsed 0.04669]\n",
      "[train] ep 24, step 10291, accu 1.00000, loss 0.01634 [elapsed 0.04814]\n",
      " [test] ep 24, step 10297, accu 0.99219 [elapsed 0.02176]\n",
      "[train] ep 25, step 10542, accu 0.98438, loss 0.09766 [elapsed 0.04825]\n",
      " [test] ep 25, step 10726, accu 0.99219 [elapsed 0.01814]\n",
      "[train] ep 26, step 10793, accu 0.98438, loss 0.08041 [elapsed 0.04466]\n",
      "[train] ep 26, step 11044, accu 0.96875, loss 0.10723 [elapsed 0.04923]\n",
      " [test] ep 26, step 11155, accu 0.99219 [elapsed 0.01887]\n",
      "[train] ep 27, step 11295, accu 0.97656, loss 0.07901 [elapsed 0.04733]\n",
      "[train] ep 27, step 11546, accu 0.96875, loss 0.11069 [elapsed 0.05158]\n",
      " [test] ep 27, step 11584, accu 0.99219 [elapsed 0.01482]\n",
      "[train] ep 28, step 11797, accu 0.95312, loss 0.13520 [elapsed 0.05054]\n",
      " [test] ep 28, step 12013, accu 0.99219 [elapsed 0.01810]\n",
      "[train] ep 29, step 12048, accu 0.96875, loss 0.08128 [elapsed 0.05057]\n",
      "[train] ep 29, step 12299, accu 0.97656, loss 0.08666 [elapsed 0.04769]\n",
      " [test] ep 29, step 12442, accu 0.99219 [elapsed 0.01608]\n",
      "[train] ep 30, step 12550, accu 0.98438, loss 0.04443 [elapsed 0.04969]\n",
      "[train] ep 30, step 12801, accu 0.95312, loss 0.09882 [elapsed 0.06940]\n",
      " [test] ep 30, step 12871, accu 0.99219 [elapsed 0.01557]\n",
      "[train] ep 31, step 13052, accu 0.96875, loss 0.10001 [elapsed 0.06678]\n",
      " [test] ep 31, step 13300, accu 0.99219 [elapsed 0.02086]\n",
      "[train] ep 32, step 13303, accu 0.96094, loss 0.10461 [elapsed 0.05083]\n",
      "[train] ep 32, step 13554, accu 0.97656, loss 0.15054 [elapsed 0.04721]\n",
      " [test] ep 32, step 13729, accu 0.99219 [elapsed 0.03779]\n",
      "[train] ep 33, step 13805, accu 0.92969, loss 0.18745 [elapsed 0.04948]\n",
      "[train] ep 33, step 14056, accu 0.94531, loss 0.09139 [elapsed 0.05134]\n",
      " [test] ep 33, step 14158, accu 0.99219 [elapsed 0.01929]\n",
      "[train] ep 34, step 14307, accu 0.96094, loss 0.11204 [elapsed 0.05185]\n",
      "[train] ep 34, step 14558, accu 0.97656, loss 0.06554 [elapsed 0.04650]\n",
      " [test] ep 34, step 14587, accu 0.99219 [elapsed 0.01597]\n",
      "[train] ep 35, step 14809, accu 0.96875, loss 0.10642 [elapsed 0.05032]\n",
      " [test] ep 35, step 15016, accu 0.99219 [elapsed 0.02009]\n",
      "[train] ep 36, step 15060, accu 0.96875, loss 0.12603 [elapsed 0.08248]\n",
      "[train] ep 36, step 15311, accu 0.98438, loss 0.05264 [elapsed 0.04864]\n",
      " [test] ep 36, step 15445, accu 0.99219 [elapsed 0.01802]\n",
      "[train] ep 37, step 15562, accu 0.96875, loss 0.08010 [elapsed 0.05068]\n",
      "[train] ep 37, step 15813, accu 0.96875, loss 0.06825 [elapsed 0.04519]\n",
      " [test] ep 37, step 15874, accu 0.99219 [elapsed 0.01657]\n",
      "[train] ep 38, step 16064, accu 0.94531, loss 0.11603 [elapsed 0.04750]\n",
      " [test] ep 38, step 16303, accu 0.99219 [elapsed 0.02453]\n",
      "[train] ep 39, step 16315, accu 0.96875, loss 0.09565 [elapsed 0.04807]\n",
      "[train] ep 39, step 16566, accu 0.97656, loss 0.07383 [elapsed 0.04826]\n",
      " [test] ep 39, step 16732, accu 0.99219 [elapsed 0.02024]\n",
      "[train] ep 40, step 16817, accu 0.96875, loss 0.11476 [elapsed 0.05159]\n",
      "[train] ep 40, step 17068, accu 0.95312, loss 0.12543 [elapsed 0.04872]\n",
      " [test] ep 40, step 17161, accu 0.99219 [elapsed 0.01715]\n",
      "[train] ep 41, step 17319, accu 0.99219, loss 0.03494 [elapsed 0.04612]\n",
      "[train] ep 41, step 17570, accu 0.97656, loss 0.06726 [elapsed 0.04859]\n",
      " [test] ep 41, step 17590, accu 0.99219 [elapsed 0.02204]\n",
      "[train] ep 42, step 17821, accu 0.96875, loss 0.11833 [elapsed 0.05559]\n",
      " [test] ep 42, step 18019, accu 0.99219 [elapsed 0.01746]\n",
      "[train] ep 43, step 18072, accu 0.94531, loss 0.14787 [elapsed 0.04809]\n",
      "[train] ep 43, step 18323, accu 0.99219, loss 0.05268 [elapsed 0.04986]\n",
      " [test] ep 43, step 18448, accu 0.99219 [elapsed 0.01731]\n",
      "[train] ep 44, step 18574, accu 0.97656, loss 0.07341 [elapsed 0.05482]\n",
      "[train] ep 44, step 18825, accu 0.98438, loss 0.02872 [elapsed 0.04726]\n",
      " [test] ep 44, step 18877, accu 0.99219 [elapsed 0.01826]\n",
      "[train] ep 45, step 19076, accu 0.96875, loss 0.05405 [elapsed 0.04886]\n",
      " [test] ep 45, step 19306, accu 0.99219 [elapsed 0.01674]\n",
      "[train] ep 46, step 19327, accu 0.96875, loss 0.07903 [elapsed 0.04950]\n",
      "[train] ep 46, step 19578, accu 0.98438, loss 0.03811 [elapsed 0.07735]\n",
      " [test] ep 46, step 19735, accu 0.99219 [elapsed 0.01813]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] ep 47, step 19829, accu 0.97656, loss 0.09635 [elapsed 0.04918]\n",
      "[train] ep 47, step 20080, accu 0.97656, loss 0.07947 [elapsed 0.05264]\n",
      " [test] ep 47, step 20164, accu 0.99219 [elapsed 0.01553]\n",
      "[train] ep 48, step 20331, accu 0.97656, loss 0.07327 [elapsed 0.04830]\n",
      "[train] ep 48, step 20582, accu 0.98438, loss 0.04497 [elapsed 0.04936]\n",
      " [test] ep 48, step 20593, accu 0.99219 [elapsed 0.01552]\n",
      "[train] ep 49, step 20833, accu 0.98438, loss 0.08219 [elapsed 0.06899]\n",
      " [test] ep 49, step 21022, accu 0.99219 [elapsed 0.01562]\n",
      "[train] ep 50, step 21084, accu 0.97656, loss 0.06770 [elapsed 0.04635]\n",
      "[train] ep 50, step 21335, accu 0.98438, loss 0.05689 [elapsed 0.05019]\n",
      " [test] ep 50, step 21451, accu 0.99219 [elapsed 0.02574]\n"
     ]
    }
   ],
   "source": [
    "train(sess, model2, 50, train_writer2, test_writer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:34:29.412212Z",
     "start_time": "2017-12-12T13:34:28.385547Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mE1214 17:49:34.696614 MainThread program.py:255] TensorBoard attempted to bind to port 6006, but it was already in use\r\n",
      "\u001b[0mTensorBoard attempted to bind to port 6006, but it was already in use\r\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir logdir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
