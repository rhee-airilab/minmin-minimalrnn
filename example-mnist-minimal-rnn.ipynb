{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:24:05.253664Z",
     "start_time": "2017-12-12T13:24:04.493518Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "from __future__ import print_function, division, absolute_import\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:24:05.372975Z",
     "start_time": "2017-12-12T13:24:05.255061Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -fr logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:24:05.505127Z",
     "start_time": "2017-12-12T13:24:05.379074Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:24:05.811621Z",
     "start_time": "2017-12-12T13:24:05.506894Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist.input_data \\\n",
    "  import read_data_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:24:06.127143Z",
     "start_time": "2017-12-12T13:24:05.813023Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = read_data_sets('./mnist', one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:24:06.131307Z",
     "start_time": "2017-12-12T13:24:06.128673Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_UNITS = 28\n",
    "NUM_HIDDEN_UNITS = 31\n",
    "BATCH_SIZE = 128\n",
    "MAX_SEQ_LEN = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:24:06.139750Z",
     "start_time": "2017-12-12T13:24:06.132935Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loop_count = mnist.train.num_examples // BATCH_SIZE\n",
    "test_loop_count  = mnist.test.num_examples // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:24:06.148736Z",
     "start_time": "2017-12-12T13:24:06.141258Z"
    }
   },
   "outputs": [],
   "source": [
    "from minimalrnn import MinimalRNNCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:24:06.235693Z",
     "start_time": "2017-12-12T13:24:06.150360Z"
    }
   },
   "outputs": [],
   "source": [
    "class MnistRnn:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, inputs, labels, use_custom_phi=False):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        dropout_rate = 0.0\n",
    "\n",
    "        training    = tf.Variable(False,name='training',dtype=tf.bool)\n",
    "        global_step = tf.Variable(1,name='global_step',dtype=tf.int64)\n",
    "        keep_prob   = tf.cond(training,lambda: 1.0 - dropout_rate,lambda: 1.0)\n",
    "\n",
    "        step_update = tf.placeholder(tf.int64,None,name='step_update')\n",
    "\n",
    "        def my_phi_initializer(inputs, num_outputs, **kwargs):\n",
    "            \n",
    "            print(('my_phi_initializer','inputs[0].shape',inputs[0].get_shape().as_list()))\n",
    "            \n",
    "            def my_phi(inputs):\n",
    "                \n",
    "                input_units = inputs.get_shape().as_list()[-1]\n",
    "                layer   = tf.reshape(inputs, [-1,input_units,1])\n",
    "\n",
    "                layer   = tf.layers.dropout(layer, rate=1.0-keep_prob, training=training)\n",
    "                layer   = tf.layers.conv1d(layer, num_outputs // 4, 3, 1) \n",
    "#                 layer   = tf.layers.batch_normalization(layer, training=training)\n",
    "                layer   = tf.nn.relu(layer)\n",
    "\n",
    "                layer   = tf.layers.dropout(layer, rate=1.0-keep_prob, training=training)\n",
    "                layer   = tf.layers.conv1d(layer, num_outputs // 2, 3, 1)\n",
    "#                 layer   = tf.layers.batch_normalization(layer, training=training)\n",
    "                layer   = tf.nn.relu(layer)\n",
    "\n",
    "                layer   = tf.layers.max_pooling1d(layer, 2, 2)\n",
    "\n",
    "                layer   = tf.layers.dropout(layer, rate=1.0-keep_prob, training=training)\n",
    "                layer   = tf.layers.conv1d(layer, num_outputs, 3, 1)\n",
    "#                 layer   = tf.layers.batch_normalization(layer, training=training)\n",
    "                layer   = tf.nn.relu(layer)\n",
    "\n",
    "                layer   = tf.layers.max_pooling1d(layer, 2, 2)\n",
    "\n",
    "                layer   = tf.reduce_max(layer, axis=1, keep_dims=True)\n",
    "\n",
    "                layer   = tf.layers.dropout(layer, rate=1.0-keep_prob, training=training)\n",
    "                layer   = tf.layers.conv1d(layer, num_outputs, 1, 1)\n",
    "#                 layer   = tf.layers.batch_normalization(layer, training=training)\n",
    "                layer   = tf.nn.relu(layer)\n",
    "\n",
    "                layer   = tf.layers.dropout(layer, rate=1.0-keep_prob, training=training)\n",
    "                layer   = tf.layers.conv1d(layer, num_outputs, 1, 1)\n",
    "#                 layer   = tf.layers.batch_normalization(layer, training=training)\n",
    "\n",
    "                layer   = tf.squeeze(layer)\n",
    "                \n",
    "                print(('my_phi','output.shape',layer.get_shape().as_list()))\n",
    "                \n",
    "                return layer\n",
    "            \n",
    "            \n",
    "            return my_phi\n",
    "\n",
    "\n",
    "        cell = MinimalRNNCell(NUM_HIDDEN_UNITS,\n",
    "                              phi_initializer=my_phi_initializer if use_custom_phi else None)\n",
    "        \n",
    "        sequence_length = [MAX_SEQ_LEN] * BATCH_SIZE\n",
    "        \n",
    "        last, states = tf.nn.dynamic_rnn(\n",
    "            cell,\n",
    "            inputs,\n",
    "            sequence_length=sequence_length,\n",
    "            dtype=tf.float32)\n",
    "        rnn_output = last[:,MAX_SEQ_LEN-1,:]\n",
    "        outputs    = tf.layers.dense(rnn_output, 10)\n",
    "        \n",
    "        loss       = tf.losses.sparse_softmax_cross_entropy(\n",
    "            labels,\n",
    "            outputs)\n",
    "\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            optimize   = tf.train.AdamOptimizer(learning_rate=0.001). \\\n",
    "                minimize(loss)\n",
    "\n",
    "        preds      = tf.argmax(outputs, axis=1)\n",
    "        errors     = tf.count_nonzero(labels - preds)\n",
    "        accuracy   = 1.0 - tf.cast(errors,tf.float32) / \\\n",
    "                        tf.cast(tf.size(preds), tf.float32)\n",
    "        \n",
    "        with tf.control_dependencies([\n",
    "            tf.assign(training, True),\n",
    "            tf.assign(global_step, step_update)\n",
    "        ]):\n",
    "            self.train_mode = tf.constant(1)\n",
    "            \n",
    "        with tf.control_dependencies([\n",
    "            tf.assign(training, False)\n",
    "        ]):\n",
    "            self.test_mode  = tf.constant(1)\n",
    "\n",
    "        self.step_update = step_update\n",
    "        self.inputs   = inputs\n",
    "        self.labels   = labels\n",
    "        self.outputs  = outputs\n",
    "        self.loss     = loss\n",
    "        self.optimize = optimize\n",
    "        self.accuracy = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:24:06.261738Z",
     "start_time": "2017-12-12T13:24:06.236746Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(sess, model, max_epochs, train_writer=None, test_writer=None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    step = sess.run(tf.train.get_global_step())\n",
    "\n",
    "    for ep in range(max_epochs):\n",
    "\n",
    "        for i in range(train_loop_count):\n",
    "\n",
    "            sess.run(model.train_mode, {model.step_update: step})\n",
    "        \n",
    "            offs = i * BATCH_SIZE\n",
    "            batch_input = mnist.train.images[offs:offs+BATCH_SIZE,:]. \\\n",
    "                            reshape([BATCH_SIZE, MAX_SEQ_LEN, INPUT_UNITS])\n",
    "            batch_label = mnist.train.labels[offs:offs+BATCH_SIZE]\n",
    "            t_start     = time.time()\n",
    "            _, loss, accuracy = sess.run(\n",
    "                [model.optimize, model.loss, model.accuracy],\n",
    "                 feed_dict = {\n",
    "                     model.inputs: batch_input,\n",
    "                     model.labels: batch_label })\n",
    "            t_elapsed     = time.time() - t_start\n",
    "            step += 1\n",
    "            \n",
    "            if train_writer:\n",
    "                summary = tf.Summary(\n",
    "                    value = [\n",
    "                        tf.Summary.Value(tag='accuracy',simple_value=accuracy),\n",
    "                        tf.Summary.Value(tag='loss',simple_value=loss),\n",
    "                        tf.Summary.Value(tag='elapsed',simple_value=t_elapsed),\n",
    "                    ])\n",
    "                train_writer.add_summary(summary, global_step=step)\n",
    "                train_writer.flush()\n",
    "                \n",
    "            if step % 251 == 0:\n",
    "                print('[train] ep {:d}, step {:d}, accu {:.5f}, loss {:.5f} [elapsed {:.5f}]'.format(\n",
    "                    ep + 1, step, accuracy, loss, t_elapsed))\n",
    "                \n",
    "        test_accuracies = []\n",
    "        \n",
    "        sess.run(model.test_mode)\n",
    "\n",
    "        for i in range(test_loop_count):\n",
    "            offs = i * BATCH_SIZE\n",
    "            batch_input = mnist.test.images[offs:offs+BATCH_SIZE,:]. \\\n",
    "                            reshape([BATCH_SIZE, MAX_SEQ_LEN, INPUT_UNITS])\n",
    "            batch_label = mnist.test.labels[offs:offs+BATCH_SIZE]\n",
    "            t_start   = time.time()\n",
    "            accuracy, = sess.run([model.accuracy],\n",
    "                                 feed_dict = {\n",
    "                                     model.inputs: batch_input,\n",
    "                                     model.labels: batch_label})\n",
    "            t_elapsed = time.time() - t_start\n",
    "            test_accuracies.append(accuracy)\n",
    "            \n",
    "            if test_writer:\n",
    "                summary = tf.Summary(\n",
    "                    value = [\n",
    "                        tf.Summary.Value(tag='accuracy',simple_value=accuracy),\n",
    "                        tf.Summary.Value(tag='elapsed',simple_value=t_elapsed),\n",
    "                    ])\n",
    "                test_writer.add_summary(summary, global_step=step)\n",
    "                test_writer.flush()\n",
    "                \n",
    "            if i % 503 == 0:\n",
    "                print(' [test] ep {:d}, step {:d}, accu {:.5f} [elapsed {:.5f}]'.format(\n",
    "                    ep + 1, step, np.mean(test_accuracies), t_elapsed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:24:06.817908Z",
     "start_time": "2017-12-12T13:24:06.264586Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "inputs_ = tf.placeholder(tf.float32,\n",
    "                         [BATCH_SIZE, MAX_SEQ_LEN, INPUT_UNITS],\n",
    "                         name='inputs')\n",
    "labels_ = tf.placeholder(tf.int64,\n",
    "                         [BATCH_SIZE],\n",
    "                         name='labels')\n",
    "\n",
    "model   = MnistRnn(inputs_, labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(gpu_options={'allow_growth':True})\n",
    "sess   = tf.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:24:08.104731Z",
     "start_time": "2017-12-12T13:24:06.822705Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_writer = tf.summary.FileWriter('logdir/train-rnn', \n",
    "                                     graph=tf.get_default_graph())\n",
    "test_writer  = tf.summary.FileWriter('logdir/test-rnn',\n",
    "                                     graph=tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:25:00.230356Z",
     "start_time": "2017-12-12T13:24:08.105917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] ep 1, step 251, accu 0.55469, loss 1.54025 [elapsed 0.00962]\n",
      " [test] ep 1, step 430, accu 0.69531 [elapsed 0.00963]\n",
      "[train] ep 2, step 502, accu 0.68750, loss 1.01582 [elapsed 0.00993]\n",
      "[train] ep 2, step 753, accu 0.75781, loss 0.77917 [elapsed 0.00969]\n",
      " [test] ep 2, step 859, accu 0.88281 [elapsed 0.00459]\n",
      "[train] ep 3, step 1004, accu 0.84375, loss 0.44357 [elapsed 0.00981]\n",
      "[train] ep 3, step 1255, accu 0.91406, loss 0.26452 [elapsed 0.01014]\n",
      " [test] ep 3, step 1288, accu 0.91406 [elapsed 0.00486]\n",
      "[train] ep 4, step 1506, accu 0.89844, loss 0.30739 [elapsed 0.01000]\n",
      " [test] ep 4, step 1717, accu 0.93750 [elapsed 0.00456]\n",
      "[train] ep 5, step 1757, accu 0.89062, loss 0.34249 [elapsed 0.00982]\n",
      "[train] ep 5, step 2008, accu 0.91406, loss 0.26912 [elapsed 0.01257]\n",
      " [test] ep 5, step 2146, accu 0.96094 [elapsed 0.00448]\n",
      "[train] ep 6, step 2259, accu 0.93750, loss 0.23653 [elapsed 0.00956]\n",
      "[train] ep 6, step 2510, accu 0.96094, loss 0.15191 [elapsed 0.00967]\n",
      " [test] ep 6, step 2575, accu 0.96094 [elapsed 0.00590]\n",
      "[train] ep 7, step 2761, accu 0.96094, loss 0.15690 [elapsed 0.01039]\n",
      " [test] ep 7, step 3004, accu 0.96875 [elapsed 0.00467]\n",
      "[train] ep 8, step 3012, accu 0.95312, loss 0.13748 [elapsed 0.00989]\n",
      "[train] ep 8, step 3263, accu 0.96094, loss 0.16084 [elapsed 0.01015]\n",
      " [test] ep 8, step 3433, accu 0.96875 [elapsed 0.00479]\n",
      "[train] ep 9, step 3514, accu 0.94531, loss 0.15449 [elapsed 0.00960]\n",
      "[train] ep 9, step 3765, accu 0.92188, loss 0.17484 [elapsed 0.00982]\n",
      " [test] ep 9, step 3862, accu 0.96875 [elapsed 0.00559]\n",
      "[train] ep 10, step 4016, accu 0.92188, loss 0.28856 [elapsed 0.01305]\n",
      "[train] ep 10, step 4267, accu 0.96875, loss 0.11800 [elapsed 0.01158]\n",
      " [test] ep 10, step 4291, accu 0.96875 [elapsed 0.00519]\n"
     ]
    }
   ],
   "source": [
    "train(sess, model, 10, train_writer, test_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:25:02.018412Z",
     "start_time": "2017-12-12T13:25:00.231543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('my_phi_initializer', 'inputs[0].shape', [28])\n",
      "('my_phi', 'output.shape', [128, 31])\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "inputs_ = tf.placeholder(tf.float32,\n",
    "                         [BATCH_SIZE, MAX_SEQ_LEN, INPUT_UNITS],\n",
    "                         name='inputs')\n",
    "labels_ = tf.placeholder(tf.int64,\n",
    "                         [BATCH_SIZE],\n",
    "                         name='labels')\n",
    "\n",
    "model   = MnistRnn(inputs_, labels_, use_custom_phi = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:25:02.021870Z",
     "start_time": "2017-12-12T13:25:02.019479Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(gpu_options={'allow_growth':True})\n",
    "sess   = tf.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:25:06.372449Z",
     "start_time": "2017-12-12T13:25:02.023074Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_writer = tf.summary.FileWriter('logdir/train-c-rnn', \n",
    "                                     graph=tf.get_default_graph())\n",
    "test_writer  = tf.summary.FileWriter('logdir/test-c-rnn',\n",
    "                                     graph=tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:34:28.384362Z",
     "start_time": "2017-12-12T13:25:06.373902Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] ep 1, step 251, accu 0.44531, loss 1.34853 [elapsed 0.07080]\n",
      " [test] ep 1, step 430, accu 0.59375 [elapsed 0.02875]\n",
      "[train] ep 2, step 502, accu 0.66406, loss 0.99989 [elapsed 0.07047]\n",
      "[train] ep 2, step 753, accu 0.72656, loss 0.94284 [elapsed 0.06955]\n",
      " [test] ep 2, step 859, accu 0.74219 [elapsed 0.01569]\n",
      "[train] ep 3, step 1004, accu 0.78906, loss 0.61635 [elapsed 0.11842]\n",
      "[train] ep 3, step 1255, accu 0.83594, loss 0.46208 [elapsed 0.09284]\n",
      " [test] ep 3, step 1288, accu 0.84375 [elapsed 0.02022]\n",
      "[train] ep 4, step 1506, accu 0.85938, loss 0.39811 [elapsed 0.07065]\n",
      " [test] ep 4, step 1717, accu 0.89062 [elapsed 0.01450]\n",
      "[train] ep 5, step 1757, accu 0.89062, loss 0.35891 [elapsed 0.07141]\n",
      "[train] ep 5, step 2008, accu 0.88281, loss 0.38823 [elapsed 0.11234]\n",
      " [test] ep 5, step 2146, accu 0.91406 [elapsed 0.01358]\n",
      "[train] ep 6, step 2259, accu 0.88281, loss 0.33796 [elapsed 0.07184]\n",
      "[train] ep 6, step 2510, accu 0.90625, loss 0.23762 [elapsed 0.07216]\n",
      " [test] ep 6, step 2575, accu 0.91406 [elapsed 0.01632]\n",
      "[train] ep 7, step 2761, accu 0.92969, loss 0.32492 [elapsed 0.07309]\n",
      " [test] ep 7, step 3004, accu 0.90625 [elapsed 0.01480]\n",
      "[train] ep 8, step 3012, accu 0.89844, loss 0.39479 [elapsed 0.07172]\n",
      "[train] ep 8, step 3263, accu 0.90625, loss 0.26919 [elapsed 0.07070]\n",
      " [test] ep 8, step 3433, accu 0.92188 [elapsed 0.01540]\n",
      "[train] ep 9, step 3514, accu 0.91406, loss 0.27688 [elapsed 0.07402]\n",
      "[train] ep 9, step 3765, accu 0.89062, loss 0.37888 [elapsed 0.06874]\n",
      " [test] ep 9, step 3862, accu 0.94531 [elapsed 0.01470]\n",
      "[train] ep 10, step 4016, accu 0.86719, loss 0.44722 [elapsed 0.08237]\n",
      "[train] ep 10, step 4267, accu 0.92969, loss 0.17180 [elapsed 0.06976]\n",
      " [test] ep 10, step 4291, accu 0.94531 [elapsed 0.01442]\n",
      "[train] ep 11, step 4518, accu 0.96094, loss 0.16586 [elapsed 0.06638]\n",
      " [test] ep 11, step 4720, accu 0.95312 [elapsed 0.01407]\n",
      "[train] ep 12, step 4769, accu 0.93750, loss 0.25395 [elapsed 0.06886]\n",
      "[train] ep 12, step 5020, accu 1.00000, loss 0.04183 [elapsed 0.06982]\n",
      " [test] ep 12, step 5149, accu 0.95312 [elapsed 0.01520]\n",
      "[train] ep 13, step 5271, accu 0.95312, loss 0.13697 [elapsed 0.07333]\n",
      "[train] ep 13, step 5522, accu 0.95312, loss 0.23173 [elapsed 0.06837]\n",
      " [test] ep 13, step 5578, accu 0.96094 [elapsed 0.01478]\n",
      "[train] ep 14, step 5773, accu 0.91406, loss 0.25507 [elapsed 0.07226]\n",
      " [test] ep 14, step 6007, accu 0.96094 [elapsed 0.01418]\n",
      "[train] ep 15, step 6024, accu 0.92969, loss 0.19230 [elapsed 0.08327]\n",
      "[train] ep 15, step 6275, accu 0.93750, loss 0.25869 [elapsed 0.08113]\n",
      " [test] ep 15, step 6436, accu 0.97656 [elapsed 0.01434]\n",
      "[train] ep 16, step 6526, accu 0.96094, loss 0.15381 [elapsed 0.07248]\n",
      "[train] ep 16, step 6777, accu 0.92969, loss 0.18652 [elapsed 0.07122]\n",
      " [test] ep 16, step 6865, accu 0.97656 [elapsed 0.01491]\n",
      "[train] ep 17, step 7028, accu 0.91406, loss 0.24274 [elapsed 0.07957]\n",
      "[train] ep 17, step 7279, accu 0.97656, loss 0.08537 [elapsed 0.07000]\n",
      " [test] ep 17, step 7294, accu 0.97656 [elapsed 0.01511]\n",
      "[train] ep 18, step 7530, accu 0.97656, loss 0.09661 [elapsed 0.07521]\n",
      " [test] ep 18, step 7723, accu 0.97656 [elapsed 0.01431]\n",
      "[train] ep 19, step 7781, accu 0.92188, loss 0.21286 [elapsed 0.06934]\n"
     ]
    }
   ],
   "source": [
    "train(sess, model, 20, train_writer, test_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T13:34:29.412212Z",
     "start_time": "2017-12-12T13:34:28.385547Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tensorboard --logdir logdir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
